{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "11a7e1e6-ab3f-49fc-9af7-c30301fb20af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Deploying and orchestrating the full workflow\n",
    "\n",
    "<img style=\"float: right; margin-left: 10px\" width=\"550px\" src=\"https://raw.githubusercontent.com/borisbanushev/CAPM_Databricks/main/lakehouseDAIWTusecases.jpg\" />\n",
    "\n",
    "All our assets are ready. We now need to define when we want our DLT pipeline to kick in and refresh the tables.\n",
    "\n",
    "One option is to switch DLT pipeline in continuous mode to have a streaming pipeline, providing near-realtime insight.\n",
    "\n",
    "An alternative is to wakeup the DLT pipeline every X hours, ingest the new data (incremental) and shut down all your compute. \n",
    "\n",
    "This is a simple configuration offering a tradeoff between uptime and ingestion latencies.\n",
    "\n",
    "In our case, we decided that the best trade-off is to ingest new data every hours:\n",
    "\n",
    "- Start the DLT pipeline to ingest new data and refresh our tables\n",
    "- Refresh the DBSQL dashboard (and potentially notify downstream applications)\n",
    "- Retrain our model to include the lastest date and capture potential behavior change\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=lakehouse&org_id=1444828305810485&notebook=%2F06-Workflow-Orchestration%2F06-Workflow-Orchestration-credit-decisioning&demo_name=lakehouse-fsi-credit&event=VIEW&path=%2F_dbdemos%2Flakehouse%2Flakehouse-fsi-credit%2F06-Workflow-Orchestration%2F06-Workflow-Orchestration-credit-decisioning&version=1&user_hash=7804490f0d3be4559d29a7b52959f461489c4ee5e35d4afc7b55f311360ac589\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "92117434-a52c-4149-ab96-6676e9de8d95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Orchestrating our Credit Decisioning pipeline with Databricks Workflows\n",
    "\n",
    "With Databricks Lakehouse we do not need any external orchestrators. We can use [Workflows](/#job/list) (available on the left menu) to orchestrate our Credit Decisioning and Scoring pipelines with just a few clicks.\n",
    "\n",
    "\n",
    "\n",
    "###  Orchestrate anything anywhere\n",
    "With workflow, you can run diverse workloads for the full data and AI lifecycle on any cloud. Orchestrate Delta Live Tables and Jobs for SQL, Spark, notebooks, dbt, ML models and more.\n",
    "\n",
    "### Simple - Fully managed\n",
    "Remove operational overhead with a fully managed orchestration service, so you can focus on your workflows not on managing your infrastructure.\n",
    "\n",
    "### Proven reliability\n",
    "Have full confidence in your workflows leveraging our proven experience running tens of millions of production workloads daily across AWS, Azure and GCP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a19b2ed8-ac72-4563-8040-47d13291f91f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### The workflow created as part of the Credit Decisioning process will orchestrate the following main tasks (as seen in the visualization below also): \n",
    "\n",
    "<p></p>\n",
    "\n",
    "* Set up data sources and feature tables\n",
    "* Ingest credit bureau data and banking internal customer data with Delta Live Tables\n",
    "* Secure your tables, lineage, audit logs, and set up encryption\n",
    "* Create Feature Engineering pipeline\n",
    "* Set up AutoML configuration and execute a run\n",
    "* Set up Batch Scoring for credit decisions\n",
    "* Create explainability and fairness for decisions\n",
    "* Set up sample Data warehousing queries and a decisions dashboard \n",
    "\n",
    "<a dbdemos-workflow-id=\"credit-job\" href=\"/#job/330117148866913\">Click here to access your Workflow job</a>, it was setup when you installed your demo.\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/borisbanushev/CAPM_Databricks/main/Workflows.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "7124194d-8c8f-4939-9e4c-67b7dc50bff2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Creating your workflow\n",
    "\n",
    "<img style=\"float: right; margin-left: 10px\" width=\"600px\" src=\"https://raw.githubusercontent.com/QuentinAmbard/databricks-demo/main/retail/resources/images/lakehouse-retail/lakehouse-retail-churn-workflow.png\" />\n",
    "\n",
    "A Databricks Workflow is composed of Tasks.\n",
    "\n",
    "Each task can trigger a specific job:\n",
    "\n",
    "* Delta Live Tables\n",
    "* SQL query / dashboard\n",
    "* Model retraining / inference\n",
    "* Notebooks\n",
    "* dbt\n",
    "* ...\n",
    "\n",
    "In this example, can see our 3 tasks:\n",
    "\n",
    "* Start the DLT pipeline to ingest new data and refresh our tables\n",
    "* Refresh the DBSQL dashboard (and potentially notify downstream applications)\n",
    "* Retrain our Churn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "b298e652-daf5-44ec-b327-94e5384699c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Monitoring your runs\n",
    "\n",
    "<img style=\"float: right; margin-left: 10px\" width=\"600px\" src=\"https://raw.githubusercontent.com/QuentinAmbard/databricks-demo/main/retail/resources/images/lakehouse-retail/lakehouse-retail-churn-workflow-monitoring.png\" />\n",
    "\n",
    "Once your workflow is created, we can access historical runs and receive alerts if something goes wrong!\n",
    "\n",
    "In the screenshot, we can see that our workflow had multiple errors, with different runtimes, and ultimately got fixed.\n",
    "\n",
    "Workflow monitoring includes errors, abnormal job duration and more advanced control!"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "06-Workflow-Orchestration-credit-decisioning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
