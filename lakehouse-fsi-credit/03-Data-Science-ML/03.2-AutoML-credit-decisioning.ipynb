{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "752c9bee-65fc-4348-a961-822eeb7f193d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Data Science on the Databricks Lakehouse\n",
    "\n",
    "## ML is key to disruption & personalization\n",
    "\n",
    "Being able to ingest and query our credit-related database is a first step, but this isn't enough to thrive in a very competitive market.\n",
    "\n",
    "Customers now expect real time personalization and new form of comunication. Modern data company achieve this with AI.\n",
    "\n",
    "<style>\n",
    ".right_box{\n",
    "  margin: 30px; box-shadow: 10px -10px #CCC; width:650px;height:300px; background-color: #1b3139ff; box-shadow:  0 0 10px  rgba(0,0,0,0.6);\n",
    "  border-radius:25px;font-size: 35px; float: left; padding: 20px; color: #f9f7f4; }\n",
    ".badge {\n",
    "  clear: left; float: left; height: 30px; width: 30px;  display: table-cell; vertical-align: middle; border-radius: 50%; background: #fcba33ff; text-align: center; color: white; margin-right: 10px}\n",
    ".badge_b { \n",
    "  height: 35px}\n",
    "</style>\n",
    "<link href='https://fonts.googleapis.com/css?family=DM Sans' rel='stylesheet'>\n",
    "<div style=\"font-family: 'DM Sans'\">\n",
    "  <div style=\"width: 500px; color: #1b3139; margin-left: 50px; float: left\">\n",
    "    <div style=\"color: #ff5f46; font-size:80px\">90%</div>\n",
    "    <div style=\"font-size:30px;  margin-top: -20px; line-height: 30px;\">\n",
    "      Enterprise applications will be AI-augmented by 2025 — IDC\n",
    "    </div>\n",
    "    <div style=\"color: #ff5f46; font-size:80px\">$10T+</div>\n",
    "    <div style=\"font-size:30px;  margin-top: -20px; line-height: 30px;\">\n",
    "       Projected business value creation by AI in 2030 — PwC\n",
    "    </div>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "  <div class=\"right_box\">\n",
    "      But a huge challenge is getting ML to work at scale!<br/><br/>\n",
    "      Most ML projects still fail before getting to production.\n",
    "  </div>\n",
    "  \n",
    "<br style=\"clear: both\">\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=lakehouse&org_id=1444828305810485&notebook=%2F03-Data-Science-ML%2F03.2-AutoML-credit-decisioning&demo_name=lakehouse-fsi-credit&event=VIEW&path=%2F_dbdemos%2Flakehouse%2Flakehouse-fsi-credit%2F03-Data-Science-ML%2F03.2-AutoML-credit-decisioning&version=1&user_hash=7804490f0d3be4559d29a7b52959f461489c4ee5e35d4afc7b55f311360ac589\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73524d4f-5cf1-4ad8-a5e2-a9ac6f6055e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### A cluster has been created for this demo\n",
    "To run this demo, just select the cluster `dbdemos-lakehouse-fsi-credit-junyi_tiong` from the dropdown menu ([open cluster configuration](https://e2-demo-field-eng.cloud.databricks.com/#setting/clusters/0922-083237-e7fg83pu/configuration)). <br />\n",
    "*Note: If the cluster was deleted after 30 days, you can re-create it with `dbdemos.create_cluster('lakehouse-fsi-credit')` or re-install the demo: `dbdemos.install('lakehouse-fsi-credit')`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82f777bf-c2b7-41b6-9afe-cb8b01d4563a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## So what makes machine learning and data science difficult?\n",
    "\n",
    "These are the top challenges we have observed companies struggle with:\n",
    "1. Inability to ingest the required data in a timely manner,\n",
    "2. Inability to properly control the access of the data,\n",
    "3. Inability to trace problems in the feature store to the raw data,\n",
    "\n",
    "... and many other data-related problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d91e8c5-0a32-4e9a-89fd-9de6046c5f57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Data-centric Machine Learning\n",
    "\n",
    "In Databricks, machine learning is not a separate product or service that needs to be \"connected\" to the data. The Lakehouse being a single, unified product, machine learning in Databricks \"sits\" on top of the data, so challenges like inability to discover and access data no longer exist.\n",
    "\n",
    "<br />\n",
    "<img src=\"https://raw.githubusercontent.com/borisbanushev/CAPM_Databricks/main/MLontheLakehouse.png\" width=\"1300px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "388565ce-7861-4a95-a34c-d5d6a16be5a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-sdk==0.36.0 mlflow==2.19.0 databricks-feature-store==0.17.0 scikit-learn==1.3.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a338ac9b-b914-487c-9b33-fe3e9ffcdc81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "5feef58d-26b1-4b6f-89b6-0faa6c762a62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Credit Scoring default prediction\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/main/images/fsi/credit_decisioning/fsi-credit-decisioning-ml-2.png\" style=\"float: right\" width=\"800px\">\n",
    "\n",
    "## Single click deployment with AutoML\n",
    "\n",
    "\n",
    "Let's see how we can now leverage the credit decisioning data to build a model predicting and explaining customer creditworthiness.\n",
    "\n",
    "We'll start by retrieving our data from the feature store and creating our training dataset.\n",
    "\n",
    "We'll then use Databricks AutoML to automatically build our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "601d1277-52f3-48ed-8e82-4999786a1407",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Loading the training dataset from the Databricks Feature Store"
    }
   },
   "outputs": [],
   "source": [
    "from databricks import feature_store\n",
    "fs = feature_store.FeatureStoreClient()\n",
    "features_set = fs.read_table(name=f\"{catalog}.{db}.credit_decisioning_features\")\n",
    "display(features_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00969aa7-45e4-4fc2-825e-3efab20c260a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating the label: \"defaulted\""
    }
   },
   "outputs": [],
   "source": [
    "credit_bureau_label = (spark.table(\"credit_bureau_gold\")\n",
    "                            .withColumn(\"defaulted\", F.when(col(\"CREDIT_DAY_OVERDUE\") > 60, 1)\n",
    "                                                      .otherwise(0))\n",
    "                            .select(\"cust_id\", \"defaulted\"))\n",
    "#As you can see, we have a fairly imbalanced dataset\n",
    "df = credit_bureau_label.groupBy('defaulted').count().toPandas()\n",
    "px.pie(df, values='count', names='defaulted', title='Credit default ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff42e216-7210-4986-8c30-a31cc0fd621f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Build our training dataset (join features and label)"
    }
   },
   "outputs": [],
   "source": [
    "training_dataset = credit_bureau_label.join(features_set, \"cust_id\", \"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f1f81b5-53b9-4ea9-bcc1-ee5990771450",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Balancing our dataset\n",
    "\n",
    "Let's downsample and upsample our dataset to improve our model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58bc3ed7-dc75-4795-b884-ef0b171355a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enable remote filtering to avoid self-join issues\n",
    "spark.conf.set(\"spark.databricks.remoteFiltering.blockSelfJoins\", \"false\")\n",
    "\n",
    "major_df = training_dataset.filter(col(\"defaulted\") == 0)\n",
    "minor_df = training_dataset.filter(col(\"defaulted\") == 1)\n",
    "\n",
    "# Duplicate the minority rows\n",
    "oversampled_df = minor_df.union(minor_df)\n",
    "\n",
    "# Downsample majority rows\n",
    "undersampled_df = major_df.sample(oversampled_df.count() / major_df.count() * 3, 42)\n",
    "\n",
    "# Combine both oversampled minority rows and undersampled majority rows\n",
    "train_df = undersampled_df.unionAll(oversampled_df).drop('cust_id').na.fill(0)\n",
    "\n",
    "# Save it as a table to be able to select it with the AutoML UI\n",
    "train_df.write.mode('overwrite').saveAsTable('credit_risk_train_df')\n",
    "train_df = spark.table('credit_risk_train_df')\n",
    "\n",
    "# Visualize the credit default ratio\n",
    "px.pie(train_df.groupBy('defaulted').count().toPandas(), values='count', names='defaulted', title='Credit default ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "e8b85486-bbd0-4f29-b630-abd512a14d58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Accelerating credit scoring model creation using MLFlow and Databricks AutoML\n",
    "\n",
    "MLFlow is an open source project allowing model tracking, packaging and deployment. Every time your Data Science team works on a model, Databricks will track all parameters and data used and will auto-log them. This ensures ML traceability and reproductibility, making it easy to know what parameters/data were used to build each model and model version.\n",
    "\n",
    "### A glass-box solution that empowers data teams without taking control away\n",
    "\n",
    "While Databricks simplifies model deployment and governance (MLOps) with MLFlow, bootstraping new ML projects can still be a long and inefficient process.\n",
    "\n",
    "Instead of creating the same boilerplate for each new project, Databricks AutoML can automatically generate state of the art models for Classifications, Regression, and Forecasting.\n",
    "\n",
    "\n",
    "<img width=\"1000\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/auto-ml-full.png\"/>\n",
    "\n",
    "\n",
    "Models can be directly deployed, or instead leverage generated notebooks to boostrap projects with best-practices, saving you weeks worth of effort.\n",
    "\n",
    "<br style=\"clear: both\">\n",
    "\n",
    "<img style=\"float: right\" width=\"600\" src=\"https://raw.githubusercontent.com/borisbanushev/CAPM_Databricks/main/MLFlowAutoML.png\"/>\n",
    "\n",
    "### Using Databricks Auto ML with our Credit Scoring dataset\n",
    "\n",
    "AutoML is available in the \"Machine Learning\" space. All we have to do is start a new AutoML Experiments and select the feature table we just created (`creditdecisioning_features`)\n",
    "\n",
    "Our prediction target is the `defaulted` column.\n",
    "\n",
    "Click on Start, and Databricks will do the rest.\n",
    "\n",
    "While this is done using the UI, you can also leverage the [python API](https://docs.databricks.com/applications/machine-learning/automl.html#automl-python-api-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fd8e114-4788-4198-95da-bb5ff594beee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"dbdemos_fsi_credit_decisioning\"\n",
    "xp_path = \"/Shared/dbdemos/experiments/lakehouse-fsi-credit-decisioning\"\n",
    "xp_name = f\"automl_credit_{datetime.now().strftime('%Y-%m-%d_%H:%M:%S')}\"\n",
    "try:\n",
    "    from databricks import automl\n",
    "    automl_run = automl.classify(\n",
    "        experiment_name = xp_name,\n",
    "        experiment_dir = xp_path,\n",
    "        dataset = train_df.sample(0.1),\n",
    "        target_col = \"defaulted\",\n",
    "        timeout_minutes = 10\n",
    "    )\n",
    "    #Make sure all users can access dbdemos shared experiment\n",
    "    DBDemos.set_experiment_permission(f\"{xp_path}/{xp_name}\")\n",
    "except Exception as e:\n",
    "    if \"cannot import name 'automl'\" in str(e) or 'method_whitelist' in str(e):\n",
    "        # Note: cannot import name 'automl' from 'databricks' likely means you're using serverless. Dbdemos doesn't support autoML serverless API - this will be improved soon.\n",
    "        # Adding a temporary workaround to make sure it works well for now - ignore this for classic run\n",
    "        automl_run = DBDemos.create_mockup_automl_run(f\"{xp_path}/{xp_name}\", train_df.sample(0.1).toPandas(), model_name=model_name, target_col=\"defaulted\")\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d7c593f-801d-4fd2-a24e-7a2942ac188e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploying our model in production\n",
    "\n",
    "Our model is now ready. We can review the notebook generated by the auto-ml run and customize if if required.\n",
    "\n",
    "For this demo, we'll consider that our model is ready and deploy it in production in our Model Registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26c2dea6-8149-4417-861d-2f361fe59f95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"dbdemos_fsi_credit_decisioning\"\n",
    "from mlflow import MlflowClient\n",
    "import mlflow\n",
    "\n",
    "#Use Databricks Unity Catalog to save our model\n",
    "mlflow.set_registry_uri('databricks-uc')\n",
    "client = MlflowClient()\n",
    "\n",
    "#Add model within our catalog\n",
    "latest_model = mlflow.register_model(f'runs:/{automl_run.best_trial.mlflow_run_id}/model', f\"{catalog}.{db}.{model_name}\")\n",
    "# Flag it as Production ready using UC Aliases\n",
    "client.set_registered_model_alias(name=f\"{catalog}.{db}.{model_name}\", alias=\"prod\", version=latest_model.version)\n",
    "#DBDemos.set_model_permission(f\"{catalog}.{db}.{model_name}\", \"ALL_PRIVILEGES\", \"account users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f4826d3-a7d1-45aa-adf4-0baae6adb832",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We just moved our automl model as production ready! \n",
    "\n",
    "Open [the dbdemos_fsi_credit_decisioning model](#mlflow/models/dbdemos_fsi_credit_decisioning) to explore its artifact and analyze the parameters used, including traceability to the notebook used for its creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18b51f5f-d0db-4bd1-8a83-5e409a5cc87d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Our model predicting default risks is now deployed in production\n",
    "\n",
    "\n",
    "So far we have:\n",
    "* ingested all required data in a single source of truth,\n",
    "* properly secured all data (including granting granular access controls, masked PII data, applied column level filtering),\n",
    "* enhanced that data through feature engineering,\n",
    "* used MLFlow AutoML to track experiments and build a machine learning model,\n",
    "* registered the model.\n",
    "\n",
    "### Next steps\n",
    "We're now ready to use our model use it for:\n",
    "\n",
    "- Batch inferences in notebook [03.3-Batch-Scoring-credit-decisioning]($./03.3-Batch-Scoring-credit-decisioning) to start using it for identifying currently underbanked customers with good credit-worthiness (**increase the revenue**) and predict current credit-owners who might default so we can prevent such defaults from happening (**manage risk**),\n",
    "- Real time inference with [03.4-model-serving-BNPL-credit-decisioning]($./03.4-model-serving-BNPL-credit-decisioning) to enable ```Buy Now, Pay Later``` capabilities within the bank.\n",
    "\n",
    "Extra: review model explainability & fairness with [03.5-Explainability-and-Fairness-credit-decisioning]($./03.5-Explainability-and-Fairness-credit-decisioning)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "03.2-AutoML-credit-decisioning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
