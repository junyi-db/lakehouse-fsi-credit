{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "d7981159-8946-408e-acd8-bf5379bfb93d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Credit Decisioning - Model Explainability and Fairness\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/main/images/fsi/credit_decisioning/fsi-credit-decisioning-ml-4.png\" style=\"float: right\" width=\"800px\">\n",
    "\n",
    "\n",
    "Machine learning (ML) models are increasingly being used in credit decisioning to automate lending processes, reduce costs, and improve accuracy. \n",
    "\n",
    "As ML models become more complex and data-driven, their decision-making processes can become opaque, making it challenging to understand how decisions are made, and to ensure that they are fair and non-discriminatory. \n",
    "\n",
    "Therefore, it is essential to develop techniques that enable model explainability and fairness in credit decisioning to ensure that the use of ML does not perpetuate existing biases or discrimination. \n",
    "\n",
    "In this context, explainability refers to the ability to understand how an ML model is making its decisions, while fairness refers to ensuring that the model is not discriminating against certain groups of people. \n",
    "\n",
    "## Ensuring model fairness for new credit customers\n",
    "\n",
    "In this example, we'll make sure that our model behaves as expected and is fair for our new customers.\n",
    "\n",
    "We'll select our existing customers not having credit (We'll flag them as `defaulted = 2`) and make sure that our model is fair and behave the same among different group of the population.\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=lakehouse&org_id=1444828305810485&notebook=%2F03-Data-Science-ML%2F03.5-Explainability-and-Fairness-credit-decisioning&demo_name=lakehouse-fsi-credit&event=VIEW&path=%2F_dbdemos%2Flakehouse%2Flakehouse-fsi-credit%2F03-Data-Science-ML%2F03.5-Explainability-and-Fairness-credit-decisioning&version=1&user_hash=7804490f0d3be4559d29a7b52959f461489c4ee5e35d4afc7b55f311360ac589\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cad67659-e06b-41bf-8753-d1938a293ff8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### A cluster has been created for this demo\n",
    "To run this demo, just select the cluster `dbdemos-lakehouse-fsi-credit-junyi_tiong` from the dropdown menu ([open cluster configuration](https://e2-demo-field-eng.cloud.databricks.com/#setting/clusters/0922-083237-e7fg83pu/configuration)). <br />\n",
    "*Note: If the cluster was deleted after 30 days, you can re-create it with `dbdemos.create_cluster('lakehouse-fsi-credit')` or re-install the demo: `dbdemos.install('lakehouse-fsi-credit')`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d09c7962-fb0b-4406-a7a6-b3b94340722f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet shap==0.46.0 mlflow==2.22.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bda82b2-7106-4f44-acaa-5864db11facd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b7e881b-1671-418f-bb7a-e8b6bf475a1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository\n",
    "\n",
    "# Configure MLflow to use Unity Catalog instead of Workspace Model Registry\n",
    "mlflow.set_registry_uri('databricks-uc')\n",
    "\n",
    "requirements_path = ModelsArtifactRepository(f\"models:/{catalog}.{db}.dbdemos_fsi_credit_decisioning@prod\").download_artifacts(artifact_path=\"requirements.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "113581b6-881e-4aa8-ab45-8bbf71d54d22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet -r $requirements_path\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e1ae5fb-6ed5-4785-aef6-5e8524cd2aef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5e0338a-cdcb-476e-a642-962bca078e1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Here we are merging several PII columns (hence we read from the ```customer_silver``` table) with the model prediction output table for visualizing them on the dashboard for end user consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5eade98-63f2-4c00-8f00-ee6a7d5de21c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_df = spark.table(\"credit_decisioning_features\")\n",
    "credit_bureau_label = spark.table(\"credit_bureau_gold\")\n",
    "customer_df = spark.table(f\"customer_silver\").select(\"cust_id\", \"gender\", \"first_name\", \"last_name\", \"email\", \"mobile_phone\")\n",
    "                   \n",
    "df = (feature_df.join(customer_df, \"cust_id\", how=\"left\")\n",
    "               .join(credit_bureau_label, \"cust_id\", how=\"left\")\n",
    "               .withColumn(\"defaulted\", F.when(col(\"CREDIT_DAY_OVERDUE\").isNull(), 2)\n",
    "                                         .when(col(\"CREDIT_DAY_OVERDUE\") > 60, 1)\n",
    "                                         .otherwise(0))\n",
    "               .drop('CREDIT_DAY_OVERDUE')\n",
    "               .fillna(0))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "873b7f9a-12ca-478d-9caa-26dbe8670c43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Model from the registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "303ff415-1e96-477c-9f3f-76a6d5939ba5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"dbdemos_fsi_credit_decisioning\"\n",
    "import mlflow\n",
    "mlflow.set_registry_uri('databricks-uc')\n",
    "\n",
    "model = mlflow.pyfunc.load_model(model_uri=f\"models:/{catalog}.{db}.{model_name}@prod\")\n",
    "features = model.metadata.get_input_schema().input_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "431d51f8-5370-4224-bbab-b8aa3cab73a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "underbanked_df = df[df.defaulted==2].toPandas() # Features for underbanked customers\n",
    "banked_df = df[df.defaulted!=2].toPandas() # Features for rest of the customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01611050-8e23-49ba-9dcc-8afbd07dd68e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature importance using Shapley values\n",
    "\n",
    "SHAP is a game-theoretic approach to explain machine learning models, providing a summary plot\n",
    "of the relationship between features and model output. Features are ranked in descending order of\n",
    "importance, and impact/color describe the correlation between the feature and the target variable.\n",
    "- Generating SHAP feature importance is a very memory intensive operation.<br />\n",
    "- To reduce the computational overhead of each trial, a single example is sampled from the underbanked set to explain.<br />\n",
    "  For more thorough results, increase the sample size of explanations, or provide your own examples to explain.\n",
    "- SHAP cannot explain models using data with nulls; if your dataset has any, both the background data and\n",
    "  examples to explain will be imputed using the mode (most frequent values). This affects the computed\n",
    "  SHAP values, as the imputed samples may not match the actual data distribution.\n",
    "\n",
    "For more information on how to read Shapley values, see the [SHAP documentation](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19415525-55f0-420c-9f23-8f0da11801f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.autolog(disable=True)\n",
    "mlflow.sklearn.autolog(disable=True)\n",
    "\n",
    "import shap\n",
    "train_sample = banked_df[features].sample(n=np.minimum(100, banked_df.shape[0]), random_state=42)\n",
    "underbanked_sample = underbanked_df.sample(n=np.minimum(100, underbanked_df.shape[0]), random_state=42)\n",
    "\n",
    "# Use Kernel SHAP to explain feature importance on the sampled rows from the validation set.\n",
    "predict = lambda x: model.predict(pd.DataFrame(x, columns=features).astype(train_sample.dtypes.to_dict()))\n",
    "\n",
    "explainer = shap.KernelExplainer(predict, train_sample, link=\"identity\")\n",
    "shap_values = explainer.shap_values(underbanked_sample[features], l1_reg=False, nsamples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "218d1034-5f31-4276-989a-5a3f79c96c1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, underbanked_sample[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46ac56f7-496c-4cd0-b2c8-097baea42ef1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Shapely values can also help for the analysis of local, instance-wise effects. \n",
    "\n",
    "We can also easily explain which feature impacted the decision for a given user. This can helps agent to understand the model an apply additional checks or control if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "530f7c6b-a082-4426-b4e8-af09d1220904",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Explain feature importance for a single customer"
    }
   },
   "outputs": [],
   "source": [
    "#shap.initjs()\n",
    "#We'll need to add shap bundle js to display nice graph\n",
    "with open(shap.__file__[:shap.__file__.rfind('/')]+\"/plots/resources/bundle.js\", 'r') as file:\n",
    "   shap_bundle_js = '<script type=\"text/javascript\">'+file.read()+';</script>'\n",
    "\n",
    "html = shap.force_plot(explainer.expected_value, shap_values[0,:], underbanked_sample[features].iloc[0,:])\n",
    "displayHTML(shap_bundle_js + html.html())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "935cab3c-2cf5-489d-a918-5eca9b4e6fca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model fairness using Shapley values\n",
    "\n",
    "In order to detect discriminatory outcomes in Machine Learning predictions, it is important to evaluate how the model treats various customer groups. This can be achieved by devising a metric, such as such as demographic parity, equal opportunity or equal odds, that defines fairness within the model. For example, when considering credit decisioning, we can compare the credit approval rates of male and female customers. In the notebook, we utilize Demographic Parity as a statistical measure of fairness, which asserts that there should be no difference between groups obtaining positive outcomes (e.g., credit approvals) in an ideal scenario. However, such perfect equality is rare, underscoring the need to monitor and address any gaps or discrepancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ee2df37-d904-4efe-9832-844313bd958b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gender_array = underbanked_df['gender'].replace({'Female':0, 'Male':1}).to_numpy()\n",
    "shap.group_difference_plot(shap_values.sum(1), \\\n",
    "                           gender_array, \\\n",
    "                           xmin=-1.0, xmax=1.0, \\\n",
    "                           xlabel=\"Demographic parity difference\\nof model output for women vs. men\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c40a2a34-3884-4109-88ce-723ec24386d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "shap_df = pd.DataFrame(shap_values, columns=features).add_suffix('_shap')\n",
    "\n",
    "shap.group_difference_plot(shap_df[['age_shap', 'tenure_months_shap']].to_numpy(), \\\n",
    "                           gender_array, \\\n",
    "                           feature_names=['age', 'tenure_months'], \n",
    "                           xmin=-0.5, xmax=0.5, \\\n",
    "                           xlabel=\"Demographic parity difference\\nof SHAP values for women vs. men\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c456a9df-84cf-4340-a04c-2ff0b49b92f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Store Data (into Delta format) for Downstream Usage\n",
    "\n",
    "Since we want to add the Explainability and Fairness assessment in the business dashboards, we will persist this data into Delta format and query it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9153d89f-67bf-404f-9eb7-53d704ad1dde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Let's load the underlying model to get the proba\n",
    "skmodel = mlflow.sklearn.load_model(model_uri=f\"models:/{catalog}.{db}.{model_name}@prod\")\n",
    "underbanked_sample['default_prob'] = skmodel.predict_proba(underbanked_sample[features])[:,1]\n",
    "underbanked_sample['prediction'] = skmodel.predict(underbanked_sample[features])\n",
    "final_df = pd.concat([underbanked_sample.reset_index(), shap_df], axis=1)\n",
    "\n",
    "final_df = spark.createDataFrame(final_df).withColumn(\"default_prob\", col(\"default_prob\").cast('double'))\n",
    "display(final_df)\n",
    "final_df.drop('CREDIT_CURRENCY', '_rescued_data') \\\n",
    "        .write.mode(\"overwrite\").option('mergeSchema', True).saveAsTable(f\"shap_explanation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d865db07-39a3-4874-b1e8-ab09b5854c1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion: the power of the Lakehouse\n",
    "\n",
    "In this demo, we've seen an end 2 end flow with the Lakehouse:\n",
    "\n",
    "- Data ingestion made simple with Delta Live Table\n",
    "- Leveraging Databricks warehouse to making credit decisions\n",
    "- Model Training with AutoML for citizen Data Scientist\n",
    "- Ability to tune our model for better results, improving our revenue\n",
    "- Ultimately, the ability to deploy and make explainable ML predictions, made possible with the full Lakehouse capabilities.\n",
    "\n",
    "[Go back to the introduction]($../00-Credit-Decisioning) or discover how to use Databricks Workflow to orchestrate this tasks: [05-Workflow-Orchestration-credit-decisioning]($../05-Workflow-Orchestration/05-Workflow-Orchestration-credit-decisioning)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "03.5-Explainability-and-Fairness-credit-decisioning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
